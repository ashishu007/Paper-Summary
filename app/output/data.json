[{"paper_name": "", "paper_category": "", "task_definition": "", "dataset_used": "", "method_definition": "", "brief_summary": ""}, {"paper_name": "Data-to-Text Generation with Content Selection and Planning.pdf", "paper_category": "Data-to-text generation using deep learning", "task_definition": "Generate natural language text from given structured representations.", "dataset_used": "RotoWire: tabular representation of data from different NFL matches with a summary of the match as gold text.", "method_definition": "Text generation decomposed into two parts: first, generate the content plan; and then use that plan to generate text.\r\n\r\nUses LSTM encoder-decoder with attention model for end-to-end training.", "brief_summary": "Model is evaluated on different automated metrics such as BLEU score and normalized Damerau-Levenshtein distance for text generation whereas precision and recall for content selection and planning.\r\n\r\nPossible drawbacks: cold-start scenario.\r\nPossible changes: Using transformers in the encoder-decoder model."}]